package org.example

import com.typesafe.config.{Config, ConfigFactory}
import org.apache.log4j.{Level, Logger}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions._

import java.time.format.DateTimeFormatter
import java.time.{LocalDate, LocalDateTime}



object SapientAssessmentQs extends App {
  // /////////////////////////////////////////////
  // Spark Session
  ///////////////////////////////////////////////
  val spark = SparkSession.builder
    .appName("sapient_questions") // optional and will be autogenerated if not specified
    .master("local[*]") // only for demo and testing purposes, use spark-submit instead
    .getOrCreate

  ///////////////////////////////////////////////
  // Spark configs and initial variables
  ///////////////////////////////////////////////
  val configs: Config = ConfigFactory.load("properties.conf")

  // Read Input Data (lazy)
  val inputFileQ1 = configs.getString("paths.q1data")
  val inputFileQ2 = configs.getString("paths.q2data")
  val inputFileQ3 = configs.getString("paths.q3data")

  // Output locations
  val outputLocQ2 = configs.getString("paths.q1outlocation")

  val inpDF1 = spark.read
    .option("header", true)
    .csv(s"$inputFileQ1")

  val inpDF2 = spark.read
    .option("header", true)
    .csv(s"$inputFileQ2")

  // Logger
  val rootLogger = Logger.getRootLogger
  rootLogger.setLevel(Level.ERROR)

  // Date and Time Variables
  val dateToday = LocalDate.now()
  val TimeToday = LocalDateTime.now()
  val dateFormat = "ddMMYYYY"
  val fmtYear = "YYYY"
  val fmtMonth = "MM"
  val fmtDay = "dd"
  val TimeStampFormat = "ddMMYYYY"

  val formattedDate = dateToday.format(DateTimeFormatter.ofPattern("ddMMYYYY"))
  val formattedTime=  TimeToday.format(DateTimeFormatter.ofPattern("YYYY-MM-dd HH:mm:ss"))
  val tsFormat: String = "2018-01-01T12:15:00Z"

  ////////////////////////////////////
  // Logic for Solution 1
  ////////////////////////////////////
  rootLogger.info("---Solution 1---")
  println("---Solution 1---")
  inpDF1.dropDuplicates("Name", "Age").show()

  ////////////////////////////////////
  // Logic for Solution 2
  ////////////////////////////////////
  val formattedDF2 = UserDefinedFunctions.changeColType(inpDF2,"timestamp","Timestamp")

  val windowSpec = Window.partitionBy("userid").orderBy("timestamp")
  val lagWindowOverTs = lag(col("timestamp"), 1).over(windowSpec)
  val leadWindowOverTs = lead(col("timestamp"), 1).over(windowSpec)

//  val inputDf2WithPrevTimestamp= formattedDF2.withColumn("prev_timestamp",lagWindowOverTs)

  val inputDf2WithDurationAndSessionId = formattedDF2
                                         .withColumn("prev_timestamp",lagWindowOverTs)
                                         .withColumn("next_timestamp",leadWindowOverTs)
                                         .withColumn("duration",(unix_timestamp(col("timestamp"))-unix_timestamp(col("prev_timestamp")))/60)
                                         .withColumn("temp_diff",(unix_timestamp(col("next_timestamp"))-unix_timestamp(col("timestamp")))/60)
                                         .withColumn("isNewSession",when(col("duration") < 30, lit(0)).otherwise(lit(1)))
                                         .withColumn("session_id",concat(lit("Session"),sum("isNewSession") over windowSpec))
                                         .na.fill(Map("duration" -> 0.0,"temp_diff" -> 0.0))
                                         .withColumn("time_spent",
                                            when(col("temp_diff") > 30, lit(30.0))
                                           .when(col("temp_diff") === 0.0, lit(30.0))
                                           .when(col("temp_diff") < 30, col("temp_diff"))
                                           )
                                         .select("userid","timestamp","time_spent","session_id")


  println("---Solution 2---")
  val q2SolutionDf = inputDf2WithDurationAndSessionId.drop("time_spent")
  q2SolutionDf.write.mode("overwrite").parquet(outputLocQ2)
  q2SolutionDf.show(false)
  println(s"---The output to Sol2 has been written to $outputLocQ2---")


  ////////////////////////////////////
  // Logic for Solution 3
  ////////////////////////////////////
  println("---Solution 3---")
  inputDf2WithDurationAndSessionId.show(false)

  val q3SolutionDf = inputDf2WithDurationAndSessionId
    .drop()
    .withColumn("event_year",year(col("timestamp")))
    .withColumn("event_month",month(col("timestamp")))
    .withColumn("event_day",dayofmonth(col("timestamp")))
  q3SolutionDf.show()                      // save this table with partitions event_year,event_month,event_day

  //Register Q3 dataframe as temporary table
  q3SolutionDf.createOrReplaceTempView("q3SolutionDf")


  //Number of sessions generated in a day.
  val countOfSessionsGeneratedInADay = spark.sql("SELECT COUNT(DISTINCT(userid,session_id)) " +
    "FROM q3SolutionDf " +
    "WHERE event_year = 2018 " +
    "AND event_month = 1 " +
    "AND event_day = 1").collect()(0)(0)

  println("Total sessions generated on 2018/01/01: " + countOfSessionsGeneratedInADay )
  //Total time spent by a user in a day

  val timeSpentByAUserInADay = spark.sql("SELECT SUM(time_spent) " +
    "FROM q3SolutionDf " +
    "WHERE userid = 'u1' " +
    "AND event_year = 2018 " +
    "AND event_month = 1 " +
    "AND event_day = 1"
  ).collect()(0)(0)

  println("Total time spent by user u1 on 2018/01/01: " + timeSpentByAUserInADay  + " mins")

  //Total time spent by a user over a month.
  val timeSpentByAUserInAMonth = spark.sql("SELECT SUM(time_spent) " +
    "FROM q3SolutionDf " +
    "WHERE userid = 'u2' " +
    "AND event_month = 1 "
  ).collect()(0)(0)

  println("Total time spent by user u2 in the month of January: " + timeSpentByAUserInAMonth  + " mins")

  //Un-persist
//  inputDf2WithPrevTimestampAndDuration.unpersist()

spark.stop()



}
